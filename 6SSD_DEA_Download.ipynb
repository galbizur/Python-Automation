{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bfb473",
   "metadata": {},
   "source": [
    "<h1>Perfect Mile download for 6 SSDs</h1>\n",
    "    <p>Plug in the information indicated below into script.  This script will fetch data for the SSDs (\"VMN1\", \"VIL1\", \"VOH1\", \"VIL2\", \"VIL3\", \"VMI1\").  Few things to note, items will download into your download folder, items take 24 hrs to clear in Perfect Mile. Eventually this script will also be coupled with Volume pulls for the same sites, and eventually work data into excel.  Also make sure any Perfect Mile Dive Deep Data AMZL misses.csv files have been deleted from your Downloads folder.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b4a15",
   "metadata": {},
   "source": [
    "This script effectively circumvents download restrictions by the server and cleans up data and introduces new columns that help determine business metrics.  In the end, you're left with a CSV file with metrics for regional comparisons to be used in a separate script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76a7451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## think in terms of start-end range\n",
    "\n",
    "yearStart = '2021'\n",
    "yearEnd = '2021'\n",
    "\n",
    "monthStart = '10'\n",
    "monthEnd = '10'\n",
    "\n",
    "dayStart = '17'\n",
    "dayEnd = '23'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211839d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "import os\n",
    "from os.path import exists\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d5cb48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "os.chdir(r\"C:\\Users\\galbizur\\Downloads\")\n",
    "\n",
    "\n",
    "driver.get('https://midway-auth.amazon.com/login?next=%2FSSO%2Fredirect%3Fredirect_uri%3Dhttps%253A%252F%252Fperfectmile-na.amazon.com%253A443%252Fdashboards%252F100%252Fregion%252FNA%252Fdaily%253Ftab%253D18495%2526start-date%253D2021-09-19%2526end-date%253D2021-09-25%2526drilldowns%253Dnode%252Fwwdea_subucket%252Ffulfillment_origin_node%2526view%253Dext_pdd%2526filter-in-location%253D%255BVIL2%255D%26client_id%3Dhttps%253A%252F%252Fperfectmile-na.amazon.com%253A443%26scope%3Dopenid%26response_type%3Did_token%26acr_values%3Dselective_mfa%26nonce%3Db251991c8984321012ca108c4861224cc58770ab76cae0631e0577725d692504%26sentry_handler_version%3DMidwayNginxModule-1.6-1%26use_sentry_key%3D1%26uid%3D7fb4e3a2-057c-4eeb-8012-e71dee5dd0e5%26for_sentry%3Dtrue%26sentry_verification%3DeyJ4NXUiOiJodHRwOlwvXC9zZW50cnktcGtpLmFtYXpvbi5jb21cL3B1YmxpY2tleVwvODU1NTkxNyIsInR5cCI6IkpXUyIsImFsZyI6IlBTMjU2In0.eyJ0eXBlIjoiU2VudHJ5VmVyaWZpY2F0aW9uIiwiZXhwIjoxNjMzMjM4MjY3fQ.YgvtnftHvCjmGbpqvFjJfTzKfvSjr4dwV789nNgQAZrMPV4RICDIhcv5rZTVsQCcB5WEyi6G5pT1QbUCOgB7EHj_XrCbUC-rv802Cev2_n60i6EbRCjjN1rIr0sc663Ihn3Z_01ww49KZPDR0-4vbjL8H0DjEAhfF3awEdibgdOlwrLXcNrCygtYfQHAtjxRsSGHLkEqQeGvtjOgIzSwAmXML3qgd17JRMseLMepHtiBWGRW3Qy0SBLa3aT_ISvHtRQGeuSjxWQUgKA2hIFKX4LTCnLmV3Zi8Oq3FnK4jnKjiwnuXVrkv4W7AB4lbbUHTOHdDtsr-4u0kE7pBkHnNQ%26kerberos%3DeyJ4NXUiOiJodHRwOlwvXC9zZW50cnktcGtpLmFtYXpvbi5jb21cL3B1YmxpY2tleVwvODU1NTkxNyIsInR5cCI6IkpXUyIsImFsZyI6IlBTMjU2In0.eyJzdWJqZWN0IjoiZ2FsYml6dXJAQU5ULkFNQVpPTi5DT00iLCJleHAiOjE2MzMyMzc2ODN9.GknYYutyC_j7xNcX-iTn3hfSDufiRH7vLvFrJOwlqiFjt3ObxVtm9RHuokRVornKD5mF385EFARzEcl-j__78P1cLQZ2OlomrxIE74pNhWiPbuCkane8RkXEjN9ItDVo8bAzZWx2xuvEdfJ7poct8QF5FkrgXhrAxkdqxpMbrAMRvGHQxSpii8Zs66iZEToJf_qyad388ebvRSLOMApHAxe8ZT1BoWY5AinbM7do3vqwQ_LzViZ71scoS9_VO4Rr9e_hMF9k2VmAulcEoyDCSDR0EKY_bm3zr0mWik3SlHt3EX2pO-uxVZIp5dHHdyiFlqObm0_ChowIRuoCcebPpQ&noauth=false&require_digital_identity=false')\n",
    "username_field = driver.find_element_by_id('user_name_field')\n",
    "username_field.send_keys(username)\n",
    "login_button = driver.find_element_by_id('user_name_btn')\n",
    "login_button.click()\n",
    "\n",
    "#password_field = driver.find_element_by_id('password_field')\n",
    "#password_field.send_keys(password)\n",
    "time.sleep(10)\n",
    "driver.fullscreen_window()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ab03965",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSD_list_CST = [\"Dive Deep Data AMZL misses VMN1.csv\", \"Dive Deep Data AMZL misses VIL1.csv\", \"Dive Deep Data AMZL misses VIL2.csv\", \"Dive Deep Data AMZL misses VIL3.csv\"]\n",
    "SSD_list_EST = [\"Dive Deep Data AMZL misses VOH1.csv\", \"Dive Deep Data AMZL misses VMI1.csv\"]\n",
    "\n",
    "SSD_list_CST_cooked = [\"VMN1.csv\", \"VIL1.csv\", \"VIL2.csv\", \"VIL3.csv\"]\n",
    "SSD_list_EST_cooked = [\"VOH1.csv\", \"VMI1.csv\"]\n",
    "\n",
    "SSD_list = [\"VMN1\", \"VIL1\", \"VOH1\", \"VIL2\", \"VIL3\", \"VMI1\"]\n",
    "SSD_csv = [\"VMN1.csv\", \"VIL1.csv\", \"VOH1.csv\", \"VIL2.csv\", \"VIL3.csv\", \"VMI1.csv\"]\n",
    "SSD_csv_raw = [\"Dive Deep Data AMZL misses VMN1.csv\", \"Dive Deep Data AMZL misses VIL1.csv\", \"Dive Deep Data AMZL misses VOH1.csv\", \"Dive Deep Data AMZL misses VIL2.csv\", \"Dive Deep Data AMZL misses VIL3.csv\", \"Dive Deep Data AMZL misses VMI1.csv\"]\n",
    "count = 0\n",
    "def downloadDEA(SSD_list = SSD_list, SSD_csv = SSD_csv):\n",
    "       \n",
    "    for x in range(6):\n",
    "        driver.get(f'https://perfectmile-na.amazon.com/blue_sky/dive_deep/bulk.csv?metric_name=total_misses_global_dea_amzl&datetime_start={yearStart}-{monthStart}-{dayStart}&datetime_end={yearEnd}-{monthEnd}-{dayEnd}T23:59:00&filters=%7B%22Location%22%3A%5B%22{SSD_list[x]}%22%5D%7D&exclude_filters=%7B%22Country%22%3A%5B%22XX%22%5D%7D&mapping=perfectmile-node-mapping-prod')\n",
    "        time.sleep(15)\n",
    "        os.rename('Dive Deep Data AMZL misses.csv', SSD_csv_raw[x])\n",
    "    driver.quit()\n",
    "\n",
    "downloadDEA()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7940dbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sunday 11:20', 'Sunday 14:20', 'Sunday 18:20', 'Monday 11:20', 'Monday 14:20', 'Monday 18:20', 'Tuesday 11:20', 'Tuesday 14:20', 'Tuesday 18:20']\n"
     ]
    }
   ],
   "source": [
    "# SSD_list_EST = [\"Dive Deep Data AMZL misses VOH1.csv\", \"Dive Deep Data AMZL misses VMI1.csv\"]\n",
    "lengthCookEST = len(SSD_list_EST)\n",
    "\n",
    "\n",
    "#In this cell we set the groundwork for identifying entries to then determine if they fit in FHD, BHD or Wednesday\n",
    "\n",
    "f_days_list = ['Sunday', 'Monday', 'Tuesday']\n",
    "f_nights_list = ['Sunday', 'Monday', 'Tuesday', 'Wednesday']\n",
    "b_days_list = ['Thursday', 'Friday', 'Saturday']\n",
    "b_nights_list = ['Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "WDN_cpt = [\"Wednesday 22:50\", \"Thursday 6:20\"]\n",
    "WDD_cpt = ['Wednesday 11:20', 'Wednesday 14:20', 'Wednesday 18:20']\n",
    "\n",
    "WDN_cpt_est = [\"Wednesday 20:50\", \"Thursday 5:20\"]\n",
    "WDD_cpt_est = ['Wednesday 10:20', 'Wednesday 13:20', 'Wednesday 17:20']\n",
    "        \n",
    "day_cpt_list = ['11:20', '14:20', '18:20']\n",
    "night_cpt_list = ['22:50', '6:20']\n",
    "\n",
    "FHD_cpt=[]\n",
    "BHD_cpt=[]\n",
    "FHN_cpt=[]\n",
    "BHN_cpt=[]\n",
    "\n",
    "def listMakerCPT(x_days_list, y_cpt_list, target_shift_cpt):\n",
    "    for days in x_days_list:\n",
    "        for cpt in y_cpt_list:\n",
    "            unionCPT = days + \" \" +cpt\n",
    "            target_shift_cpt.append(unionCPT)            \n",
    "            \n",
    "listMakerCPT(f_days_list, day_cpt_list, FHD_cpt)\n",
    "listMakerCPT(b_days_list, day_cpt_list, BHD_cpt)\n",
    "listMakerCPT(f_nights_list, night_cpt_list, FHN_cpt)\n",
    "listMakerCPT(b_nights_list, night_cpt_list, BHN_cpt)\n",
    "\n",
    "FHN_cpt.remove('Sunday 6:20')\n",
    "FHN_cpt.remove('Wednesday 22:50')\n",
    "print(FHD_cpt)\n",
    "#print(FHN_cpt)\n",
    "#print(BHD_cpt)\n",
    "\n",
    "BHN_cpt.remove('Thursday 6:20')\n",
    "BHN_cpt.remove('Sunday 22:50')\n",
    "#print(BHN_cpt)\n",
    "\n",
    "def shiftCalc(x):\n",
    "    if (x in FHD_cpt):\n",
    "        return \"FHD\"\n",
    "    elif (x in FHN_cpt):\n",
    "        return \"FHN\"\n",
    "    elif (x in BHD_cpt):\n",
    "        return \"BHD\"\n",
    "    elif (x in BHN_cpt):\n",
    "        return \"BHN\"\n",
    "    elif (x in WDD_cpt):\n",
    "        return \"WDD\"\n",
    "    elif (x in WDN_cpt):\n",
    "        return \"WDN\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cefc566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cookEST(SSD_list_EST = SSD_list_EST, SSD_list_EST_cooked = SSD_list_EST_cooked):\n",
    "    for x in range(lengthCookEST): #using len for future proofing, in the event more SSD's are added\n",
    "        # for loop will clean all adjust times for CST for SSD's residing in EST region\n",
    "        df = pd.read_csv(SSD_list_EST[x], usecols=['tracking_id','node_id','bucket', 'expected_pickup_date'])\n",
    "        #os.remove(\"Dive Deep Data AMZL misses.csv\")\n",
    "        df['expected_pickup_date'] = pd.to_datetime(df['expected_pickup_date'])\n",
    "        df['CPT_CST']=df['expected_pickup_date']-timedelta(hours=4)\n",
    "        temp = pd.DatetimeIndex(df['CPT_CST'])\n",
    "        df['Day'] = df['CPT_CST'].dt.day_name()\n",
    "        df['date'] = df['CPT_CST']\n",
    "        df['date'] = temp.date\n",
    "        df['time'] = temp.time\n",
    "        df['hr']= temp.hour\n",
    "        df['min']=temp.minute\n",
    "        df['WeekDay_CPT'] = df['Day'].astype(str)+ \" \" + df['hr'].astype(str) + \":\" + df['min'].astype(str)\n",
    "        df['CPT_time'] = df['hr'].astype(str) + \":\" + df['min'].astype(str)\n",
    "        df['shift_half'] = df['WeekDay_CPT'].apply(shiftCalc)\n",
    "        df['shift_half_CPT'] = df['shift_half']+\" \"+df['CPT_time']\n",
    "        df.drop_duplicates(subset=['tracking_id'], inplace=True)\n",
    "        df.drop(columns=['tracking_id', 'Day', 'date', 'time', 'hr', 'min', 'WeekDay_CPT'], inplace=True)\n",
    "        df_filtered = df[(df['bucket']=='Items Missing')|(df['bucket']=='Late Dispatch')|(df['bucket']=='Late Induct')|(df['bucket']=='Not Dispatched')|(df['bucket']== 'Not Inducted')]\n",
    "        df_raw2 = pd.DataFrame(df_filtered)\n",
    "\n",
    "        df_raw2 = df_raw2.reset_index()\n",
    "        df_raw2.drop(columns=['expected_pickup_date','index'], inplace=True)\n",
    "        df_raw2.to_csv(SSD_list_EST_cooked[x])\n",
    "cookEST()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73ed5cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cookCST(SSD_list_CST = SSD_list_CST, SSD_list_CST_cooked = SSD_list_CST_cooked):\n",
    "    for x in range(len(SSD_list_CST)):\n",
    "    #same as above but for CST local time SSDs    \n",
    "        df = pd.read_csv(SSD_list_CST[x], usecols=['tracking_id','node_id','bucket', 'expected_pickup_date'])\n",
    "        #os.remove(\"Dive Deep Data AMZL misses.csv\")\n",
    "        df['expected_pickup_date'] = pd.to_datetime(df['expected_pickup_date'])\n",
    "        df['CPT_CST']=df['expected_pickup_date']-timedelta(hours=5)\n",
    "        temp = pd.DatetimeIndex(df['CPT_CST'])\n",
    "        df['Day'] = df['CPT_CST'].dt.day_name()\n",
    "        df['date'] = df['CPT_CST']\n",
    "        df['date'] = temp.date\n",
    "        df['time'] = temp.time\n",
    "        df['hr']= temp.hour\n",
    "        df['min']=temp.minute\n",
    "        df['WeekDay_CPT'] = df['Day'].astype(str)+ \" \" + df['hr'].astype(str) + \":\" + df['min'].astype(str)\n",
    "        df['CPT_time'] = df['hr'].astype(str) + \":\" + df['min'].astype(str)\n",
    "        df['shift_half'] = df['WeekDay_CPT'].apply(shiftCalc)\n",
    "        df['shift_half_CPT'] = df['shift_half']+\" \"+df['CPT_time']\n",
    "        df.drop_duplicates(subset=['tracking_id'], inplace=True)\n",
    "        df.drop(columns=['tracking_id', 'Day', 'date', 'time', 'hr', 'min', 'WeekDay_CPT'], inplace=True)\n",
    "        df_filtered = df[(df['bucket']=='Items Missing')|(df['bucket']=='Late Dispatch')|(df['bucket']=='Late Induct')|(df['bucket']=='Not Dispatched')|(df['bucket']== 'Not Inducted')]\n",
    "        df_raw2 = pd.DataFrame(df_filtered)\n",
    "\n",
    "        df_raw2 = df_raw2.reset_index()\n",
    "        df_raw2.drop(columns=['expected_pickup_date','index'], inplace=True)\n",
    "        df_raw2.to_csv(SSD_list_CST_cooked[x])\n",
    "        \n",
    "cookCST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5169613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_combine():\n",
    "    df0 = pd.read_csv(SSD_csv[0])\n",
    "    df1 = pd.read_csv(SSD_csv[1])\n",
    "    df2 = pd.read_csv(SSD_csv[2])\n",
    "    df3 = pd.read_csv(SSD_csv[3])\n",
    "    df4 = pd.read_csv(SSD_csv[4])\n",
    "    df5 = pd.read_csv(SSD_csv[5])\n",
    "    \n",
    "    df_combined = pd.concat([df0, df1, df2, df3, df4, df5], axis=0)\n",
    "    \n",
    "    return df_combined\n",
    "df_total = df_combine()\n",
    "df_total.drop(columns=[df_total.columns[0]], inplace=True)\n",
    "df_total = df_total.reset_index()\n",
    "\n",
    "df_total.drop(columns=[df_total.columns[0]], inplace=True)\n",
    "\n",
    "df_total.to_csv(\"DEA_Misses_Complete.csv\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
